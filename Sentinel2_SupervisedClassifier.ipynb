{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel 2 Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikitlearn supervised classification for exploring Greenland's Dark Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written by Joseph Cook, University of Sheffield (UK), June 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code provides functions for reading in directional reflectance data obtained via ground spectroscopy, reformatting into a pandas dataframe of features and labels, optimising and training a series of machine learning algorithms on the ground spectra then using the trained model to predict the surface type in each pixel of a Sentinel-2 image. The Sentinel-2 image has been preprocessed using ESA Sen2Cor command line algorithm to convert to surface reflectance before being saved as a multiband TIF which is then loaded here. Three individual sub-areas within the main image are selected for analysis, maximising the glaciated area included in the study.\n",
    "#### This script is divided into several functions. The first preprocesses the raw data into a format appropriate for machine learning. The raw hyperspectral data is first organised into separate pandas dataframes for each surface class. The data is then reduced down to the reflectance at the nine key wavelengths and the remaining data discarded. The dataset is then arranged into columns with one column per wavelength and a separate column for the surface class. The dataset's features are the reflectance at each wavelength, and the labels are the surface types. The dataframes for each surface type are merged into one large dataframe and then the labels are removed and saved as a separate dataframe. XX contains all the data features, YY contains the labels only. No scaling of the data is required because the reflectance is already normalised between 0 and 1 by the spectrometer.\n",
    "#### Each classifier is trained and the performance on the training set is reported. The user can define which performance measure is most important, and the best performing classifier according to the chosen metric is automatically selected as the final model. That model is then evaluated on the test set and used to classify each pixel in the UAV image. The classified image is displayed and the spatial statistics calculated.\n",
    "#### NB The classifier can also be loaded in from a joblib save file - in this case omit the call to the optimise_train_model() function and simply load the trained classifier into the workspace with the variable name 'clf'. Run the other functions as normal.\n",
    "#### The trained classifier can also be exported to a joblib savefile by running the save_classifier() function,enabling the trained model to be replicated in other scripts.\n",
    "#### The albedo of each classified pixel is then calculated from the reflectance at each individual wavelength using the narrowband-broadband conversion of Liang et al (2002), creating a final dataframe containing broadband albedo and surface type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing, cross_validation, neighbors, svm\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score, precision_score\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gdal\n",
    "import rasterio\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import training data and images to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "HCRF_file = '//home//joe//Code//HCRF_master_machine_snicar.csv'\n",
    "\n",
    "Seninel_jp2s = ['/media/joe/FDB2-2F9B/2016_Sentinel/B02_20m.jp2', '/media/joe/FDB2-2F9B/2016_Sentinel/B03_20m.jp2', '/media/joe/FDB2-2F9B/2016_Sentinel/B04_20m.jp2', '/media/joe/FDB2-2F9B/2016_Sentinel/B05_20m.jp2',\n",
    "        '/media/joe/FDB2-2F9B/2016_Sentinel/B06_20m.jp2','/media/joe/FDB2-2F9B/2016_Sentinel/B07_20m.jp2','/media/joe/FDB2-2F9B/2016_Sentinel/B8A_20m.jp2',\n",
    "        '/media/joe/FDB2-2F9B/2016_Sentinel/B11_20m.jp2','/media/joe/FDB2-2F9B/2016_Sentinel/B12_20m.jp2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(HCRF_file,plot_spectra=True):\n",
    "\n",
    "    # Read in raw HCRF data to DataFrame. This version pulls in HCRF data from 2016 and 2017\n",
    "    hcrf_master = pd.read_csv(HCRF_file)\n",
    "    HA_hcrf = pd.DataFrame()\n",
    "    LA_hcrf = pd.DataFrame()\n",
    "    CI_hcrf = pd.DataFrame()\n",
    "    CC_hcrf = pd.DataFrame()\n",
    "    WAT_hcrf = pd.DataFrame()\n",
    "    SN_hcrf = pd.DataFrame()\n",
    "    \n",
    "    # Group filenames from master csv by surface class (HA = heavy algal bloom, LA = light algal bloom,\n",
    "    # CI = clean ice, SN= snow, WAT = water, CC = cryoconite)\n",
    "    \n",
    "    HAsites = ['13_7_SB2','13_7_SB4','14_7_S5','14_7_SB1','14_7_SB5','14_7_SB10',\n",
    "    '15_7_SB3','21_7_SB1','21_7_SB7','22_7_SB4','22_7_SB5','22_7_S3','22_7_S5',\n",
    "    '23_7_SB3','23_7_SB5','23_7_S3','23_7_SB4','24_7_SB2','HA_1', 'HA_2','HA_3',\n",
    "    'HA_4','HA_5','HA_6','HA_7','HA_8','HA_10','HA_11','HA_12','HA_13','HA_14',\n",
    "    'HA_15','HA_16','HA_17','HA_18','HA_19','HA_20','HA_21','HA_22','HA_24',\n",
    "    'HA_25','HA_26','HA_27','HA_28','HA_29','HA_30','HA_31','13_7_S2','14_7_SB9',\n",
    "    'MA_11','MA_14','MA_15','MA_17','21_7_SB2','22_7_SB1','MA_4','MA_7','MA_18',\n",
    "    '27_7_16_SITE3_WMELON1', '27_7_16_SITE3_WMELON3','27_7_16_SITE2_ALG1',\n",
    "    '27_7_16_SITE2_ALG2', '27_7_16_SITE2_ALG3', '27_7_16_SITE2_ICE3','27_7_16_SITE2_ICE5',\n",
    "    '27_7_16_SITE3_ALG4','5_8_16_site2_ice7','5_8_16_site3_ice2', '5_8_16_site3_ice3',\n",
    "    '5_8_16_site3_ice5', '5_8_16_site3_ice6', '5_8_16_site3_ice7',\n",
    "    '5_8_16_site3_ice8', '5_8_16_site3_ice9']\n",
    "    \n",
    "    LAsites = ['14_7_S2','14_7_S3','14_7_SB2','14_7_SB3','14_7_SB7','15_7_S2','15_7_SB4',\n",
    "    '20_7_SB1','20_7_SB3','21_7_S1','21_7_S5','21_7_SB4','22_7_SB2','22_7_SB3','22_7_S1',\n",
    "    '23_7_S1','23_7_S2','24_7_S2','MA_1','MA_2','MA_3','MA_5','MA_6','MA_8','MA_9',\n",
    "    'MA_10','MA_12','MA_13','MA_16','MA_19','13_7_S1','13_7_S3','14_7_S1','15_7_S1',\n",
    "    '15_7_SB2','20_7_SB2','21_7_SB5','21_7_SB8','25_7_S3','5_8_16_site2_ice10','5_8_16_site2_ice5',\n",
    "    '5_8_16_site2_ice9','27_7_16_SITE3_WHITE3']\n",
    "    \n",
    "    CIsites =['21_7_S4','13_7_SB3','15_7_S4','15_7_SB1','15_7_SB5','21_7_S2',\n",
    "    '21_7_SB3','22_7_S2','22_7_S4','23_7_SB1','23_7_SB2','23_7_S4',\n",
    "    'WI_1','WI_2','WI_4','WI_5','WI_6','WI_7','WI_9','WI_10','WI_11',\n",
    "    'WI_12','WI_13','27_7_16_SITE3_WHITE1', '27_7_16_SITE3_WHITE2', \n",
    "    '27_7_16_SITE2_ICE2','27_7_16_SITE2_ICE4','27_7_16_SITE2_ICE6',\n",
    "    '5_8_16_site2_ice1',  '5_8_16_site2_ice2', '5_8_16_site2_ice3', '5_8_16_site2_ice4',\n",
    "    '5_8_16_site2_ice6','5_8_16_site2_ice8','5_8_16_site3_ice1','5_8_16_site3_ice4']\n",
    "    \n",
    "    CCsites = ['DISP1','DISP2','DISP3','DISP4','DISP5','DISP6','DISP7','DISP8','DISP9','DISP10',\n",
    "               'DISP11','DISP12','DISP13','DISP14','27_7_16_SITE3_DISP1', '27_7_16_SITE3_DISP3',]\n",
    "    \n",
    "    WATsites = ['21_7_SB5','21_7_SB8','WAT_1','WAT_3','WAT_6']\n",
    "    \n",
    "    SNsites = ['14_7_S4','14_7_SB6','14_7_SB8','17_7_SB2','SNICAR100','SNICAR200',\n",
    "               'SNICAR300','SNICAR400','SNICAR500','SNICAR600','SNICAR700','SNICAR800','SNICAR900','SNICAR1000',\n",
    "               '27_7_16_KANU_','27_7_16_SITE2_1','5_8_16_site1_snow10', '5_8_16_site1_snow2', '5_8_16_site1_snow3',\n",
    "               '5_8_16_site1_snow4', '5_8_16_site1_snow6',\n",
    "               '5_8_16_site1_snow7', '5_8_16_site1_snow9']\n",
    "    \n",
    "    \n",
    "    # Create dataframes for classification algorithm. organise each dataframe into columns for features and labels,\n",
    "    # then concatenate into a single dataframe\n",
    "    \n",
    "    for i in HAsites:\n",
    "        hcrf_HA = np.array(hcrf_master[i])\n",
    "        HA_hcrf['{}'.format(i)] = hcrf_HA\n",
    "    \n",
    "    for ii in LAsites:\n",
    "        hcrf_LA = np.array(hcrf_master[ii])\n",
    "        LA_hcrf['{}'.format(ii)] = hcrf_LA\n",
    "         \n",
    "    for iii in CIsites:   \n",
    "        hcrf_CI = np.array(hcrf_master[iii])\n",
    "        CI_hcrf['{}'.format(iii)] = hcrf_CI   \n",
    "    \n",
    "    for iv in CCsites:   \n",
    "        hcrf_CC = np.array(hcrf_master[iv])\n",
    "        CC_hcrf['{}'.format(iv)] = hcrf_CC   \n",
    "    \n",
    "    for v in WATsites:   \n",
    "        hcrf_WAT = np.array(hcrf_master[v])\n",
    "        WAT_hcrf['{}'.format(v)] = hcrf_WAT  \n",
    "\n",
    "    for vi in SNsites:\n",
    "        hcrf_SN = np.array(hcrf_master[vi])\n",
    "        SN_hcrf['{}'.format(vi)] = hcrf_SN \n",
    "    \n",
    "    \n",
    "    # Plot the training data with each surface class in a spearate plot (only if plot_spectra is set to True in function call)\n",
    "    \n",
    "    if plot_spectra:\n",
    "        WL = np.arange(350,2501,1)\n",
    "        HA_hcrf.plot(x = WL,legend=None),plt.ylim(0,1.2),plt.title('HA'),plt.xlabel('Wavelength (nm)'),plt.ylabel('HCRF')\n",
    "        LA_hcrf.plot(x = WL,legend=None),plt.ylim(0,1.2),plt.title('LA'),plt.xlabel('Wavelength (nm)'),plt.ylabel('HCRF')\n",
    "        CI_hcrf.plot(x = WL,legend=None),plt.ylim(0,1.2),plt.title('CI'),plt.xlabel('Wavelength (nm)'),plt.ylabel('HCRF')\n",
    "        CC_hcrf.plot(x = WL,legend=None),plt.ylim(0,1.2),plt.title('CC'),plt.xlabel('Wavelength (nm)'),plt.ylabel('HCRF')\n",
    "        WAT_hcrf.plot(x = WL,legend=None),plt.ylim(0,1.2),plt.title('WAT'),plt.xlabel('Wavelength (nm)'),plt.ylabel('HCRF')\n",
    "        SN_hcrf.plot(x = WL,legend=None),plt.ylim(0,1.2),plt.title('SN'),plt.xlabel('Wavelength (nm)'),plt.ylabel('HCRF')\n",
    "    # Make dataframe with column for label, columns for reflectancxe at key wavelengths\n",
    "    # select wavelengths to use - currently set to 8 Sentnel 2 bands\n",
    "    \n",
    "    X = pd.DataFrame()\n",
    "\n",
    "    X['R140'] = np.array(HA_hcrf.iloc[140])\n",
    "    X['R210'] = np.array(HA_hcrf.iloc[210])\n",
    "    X['R315'] = np.array(HA_hcrf.iloc[315])\n",
    "    X['R355'] = np.array(HA_hcrf.iloc[355])\n",
    "    X['R390'] = np.array(HA_hcrf.iloc[390])\n",
    "    X['R433'] = np.array(HA_hcrf.iloc[433])\n",
    "    X['R515'] = np.array(HA_hcrf.iloc[515])\n",
    "    X['R1260'] = np.array(HA_hcrf.iloc[1260])\n",
    "    X['R1840'] = np.array(HA_hcrf.iloc[1840])\n",
    "    \n",
    "    X['label'] = 'HA'\n",
    "    \n",
    "    \n",
    "    Y = pd.DataFrame()\n",
    "    Y['R140'] = np.array(LA_hcrf.iloc[140])\n",
    "    Y['R210'] = np.array(LA_hcrf.iloc[210])\n",
    "    Y['R315'] = np.array(LA_hcrf.iloc[315])\n",
    "    Y['R355'] = np.array(LA_hcrf.iloc[355])\n",
    "    Y['R390'] = np.array(LA_hcrf.iloc[390])\n",
    "    Y['R433'] = np.array(LA_hcrf.iloc[433])\n",
    "    Y['R515'] = np.array(LA_hcrf.iloc[515])\n",
    "    Y['R1260'] = np.array(LA_hcrf.iloc[1260])\n",
    "    Y['R1840'] = np.array(LA_hcrf.iloc[1840])\n",
    "    \n",
    "    Y['label'] = 'LA'\n",
    "    \n",
    "    \n",
    "    Z = pd.DataFrame()\n",
    "    \n",
    "    Z['R140'] = np.array(CI_hcrf.iloc[140])\n",
    "    Z['R210'] = np.array(CI_hcrf.iloc[210])\n",
    "    Z['R315'] = np.array(CI_hcrf.iloc[315])\n",
    "    Z['R355'] = np.array(CI_hcrf.iloc[355])\n",
    "    Z['R390'] = np.array(CI_hcrf.iloc[390])\n",
    "    Z['R433'] = np.array(CI_hcrf.iloc[433])\n",
    "    Z['R515'] = np.array(CI_hcrf.iloc[515])\n",
    "    Z['R1260'] = np.array(CI_hcrf.iloc[1260])\n",
    "    Z['R1840'] = np.array(CI_hcrf.iloc[1840])\n",
    "    \n",
    "    Z['label'] = 'CI'\n",
    "    \n",
    "    \n",
    "    P = pd.DataFrame()\n",
    "    \n",
    "    P['R140'] = np.array(CC_hcrf.iloc[140])\n",
    "    P['R210'] = np.array(CC_hcrf.iloc[210])\n",
    "    P['R315'] = np.array(CC_hcrf.iloc[315])\n",
    "    P['R355'] = np.array(CC_hcrf.iloc[355])\n",
    "    P['R390'] = np.array(CC_hcrf.iloc[390])\n",
    "    P['R433'] = np.array(CC_hcrf.iloc[433])\n",
    "    P['R515'] = np.array(CC_hcrf.iloc[515])\n",
    "    P['R1260'] = np.array(CC_hcrf.iloc[1260])\n",
    "    P['R1840'] = np.array(CC_hcrf.iloc[1840])\n",
    "    \n",
    "    P['label'] = 'CC'\n",
    "    \n",
    "    \n",
    "    Q = pd.DataFrame()\n",
    "    Q['R140'] = np.array(WAT_hcrf.iloc[140])\n",
    "    Q['R210'] = np.array(WAT_hcrf.iloc[210])\n",
    "    Q['R315'] = np.array(WAT_hcrf.iloc[315])\n",
    "    Q['R355'] = np.array(WAT_hcrf.iloc[355])\n",
    "    Q['R390'] = np.array(WAT_hcrf.iloc[390])\n",
    "    Q['R433'] = np.array(WAT_hcrf.iloc[433])\n",
    "    Q['R515'] = np.array(WAT_hcrf.iloc[515])\n",
    "    Q['R1260'] = np.array(WAT_hcrf.iloc[1260])\n",
    "    Q['R1840'] = np.array(WAT_hcrf.iloc[1840])\n",
    "    \n",
    "    Q['label'] = 'WAT'\n",
    "    \n",
    "    R = pd.DataFrame()\n",
    "    R['R140'] = np.array(SN_hcrf.iloc[140])\n",
    "    R['R210'] = np.array(SN_hcrf.iloc[210])\n",
    "    R['R315'] = np.array(SN_hcrf.iloc[315])\n",
    "    R['R355'] = np.array(SN_hcrf.iloc[355])\n",
    "    R['R390'] = np.array(SN_hcrf.iloc[390])\n",
    "    R['R433'] = np.array(SN_hcrf.iloc[433])\n",
    "    R['R515'] = np.array(SN_hcrf.iloc[515])\n",
    "    R['R1260'] = np.array(SN_hcrf.iloc[1260])\n",
    "    R['R1840'] = np.array(SN_hcrf.iloc[1840])\n",
    "    \n",
    "    R['label'] = 'SN'\n",
    "\n",
    "    X = X.append(Y,ignore_index=True)\n",
    "    X = X.append(Z,ignore_index=True)\n",
    "    X = X.append(P,ignore_index=True)\n",
    "    X = X.append(Q,ignore_index=True)\n",
    "    X = X.append(R,ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Create features and labels (XX = features - all data but no labels, YY = labels only)\n",
    "    XX = X.drop(['label'],1)\n",
    "    YY = X['label']\n",
    "        \n",
    "    return X, XX, YY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call function\n",
    "X,XX,YY = create_dataset(HCRF_file,plot_spectra=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a range of supervised classification algorithms, determine the best performing and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimise_train_model(X,XX,YY, error_selector, test_size = 0.3, plot_all_conf_mx = True):\n",
    "    \n",
    "    # Function splits the data into training and test sets, then tests the \n",
    "    # performance of a range of models on the training data. The final model \n",
    "    # selected is then evaluated using the test set. The function automatically\n",
    "    # selects the model that performs best on the training sets. All \n",
    "    # performance metrics are printed to ipython. The performance metric \n",
    "    # to use to select the model is determined in the function call. Options \n",
    "    # for error_selector are: 'accuracy', 'F1', 'recall', 'precision', and \n",
    "    # 'average_all_metric' \n",
    "    # The option 'plot_all_conf_mx' can be se to True or False. If True, the \n",
    "    # train set confusion matrices will be plotted for all models. If False,\n",
    "    # only the final model confusion matrix will be plotted.\n",
    "    # X, XX, YY are the datasets with and without labels.\n",
    "\n",
    "    # split data into test and train sets.\n",
    "    X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(XX,YY,test_size = test_size)\n",
    "    \n",
    "    # test different classifers and report performance metrics using traning data only\n",
    "    \n",
    "    # 1. Try Naive Bayes\n",
    "    clf_NB = GaussianNB()\n",
    "    clf_NB.fit(X_train,Y_train)\n",
    "    accuracy_NB = clf_NB.score(X_train,Y_train) #calculate accuracy\n",
    "    Y_predict_NB = clf_NB.predict(X_train) # make nre prediction\n",
    "    conf_mx_NB = confusion_matrix(Y_train,Y_predict_NB) # calculate confusion matrix\n",
    "    recall_NB = recall_score(Y_train,Y_predict_NB,average=\"weighted\")\n",
    "    f1_NB = f1_score(Y_train, Y_predict_NB, average=\"weighted\") # calculate f1 score\n",
    "    precision_NB = precision_score(Y_train,Y_predict_NB, average = 'weighted')\n",
    "    average_metric_NB = (accuracy_NB+recall_NB+f1_NB)/3\n",
    "    \n",
    "    # 2. Try K-nearest neighbours\n",
    "    clf_KNN = neighbors.KNeighborsClassifier()\n",
    "    clf_KNN.fit(X_train,Y_train)\n",
    "    accuracy_KNN = clf_KNN.score(X_train,Y_train)\n",
    "    Y_predict_KNN = clf_KNN.predict(X_train)\n",
    "    conf_mx_KNN = confusion_matrix(Y_train,Y_predict_KNN)\n",
    "    recall_KNN = recall_score(Y_train,Y_predict_KNN,average=\"weighted\")\n",
    "    f1_KNN = f1_score(Y_train, Y_predict_KNN, average=\"weighted\")\n",
    "    precision_KNN = precision_score(Y_train,Y_predict_KNN, average = 'weighted')\n",
    "    average_metric_KNN = (accuracy_KNN + recall_KNN + f1_KNN)/3\n",
    "    \n",
    "    # 3. Try support Vector Machine with best params calculated using\n",
    "    # GridSearch cross validation optimisation\n",
    "    tuned_parameters = [\n",
    "        {'kernel': ['linear'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "                     'C': [0.1, 1, 10, 100, 1000, 10000]},\n",
    "                    {'kernel': ['rbf'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "                     'C': [0.1, 1, 10, 100, 1000, 10000]},\n",
    "                    {'kernel':['poly'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "                     'C':[0.1,1,10,100,1000,10000]},\n",
    "                    {'kernel':['sigmoid'],'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "                     'C':[0.1,1,10,100,1000,10000]}\n",
    "                    ]\n",
    "    \n",
    "    clf_svm = GridSearchCV(svm.SVC(C=1), tuned_parameters, cv=3)\n",
    "    clf_svm.fit(X_train, Y_train)\n",
    "    \n",
    "    print()\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(clf_svm.best_params_)\n",
    "    print() #line break\n",
    "    \n",
    "    kernel = clf_svm.best_estimator_.get_params()['kernel']\n",
    "    C = clf_svm.best_estimator_.get_params()['C']\n",
    "    gamma = clf_svm.best_estimator_.get_params()['gamma']\n",
    "    \n",
    "    clf_svm = svm.SVC(kernel=kernel, C=C, gamma = gamma,probability=True)\n",
    "    clf_svm.fit(X_train,Y_train)\n",
    "    accuracy_svm = clf_svm.score(X_train,Y_train)\n",
    "    Y_predict_svm = clf_svm.predict(X_train)\n",
    "    conf_mx_svm = confusion_matrix(Y_train,Y_predict_svm)\n",
    "    recall_svm = recall_score(Y_train,Y_predict_svm,average=\"weighted\")\n",
    "    f1_svm = f1_score(Y_train, Y_predict_svm, average=\"weighted\")\n",
    "    precision_svm = precision_score(Y_train,Y_predict_svm, average = 'weighted')\n",
    "    average_metric_svm = (accuracy_svm + recall_svm + f1_svm)/3\n",
    "\n",
    "    # 4. Try  a random forest classifier\n",
    "    clf_RF = RandomForestClassifier(n_estimators = 1000, max_leaf_nodes = 16,n_jobs=-1)\n",
    "    clf_RF.fit(X_train,Y_train)\n",
    "    accuracy_RF = clf_RF.score(X_train,Y_train)\n",
    "    Y_predict_RF = clf_RF.predict(X_train)\n",
    "    conf_mx_RF = confusion_matrix(Y_train,Y_predict_RF)\n",
    "    recall_RF = recall_score(Y_train,Y_predict_RF,average=\"weighted\")\n",
    "    f1_RF = f1_score(Y_train, Y_predict_RF, average=\"weighted\")\n",
    "    precision_RF = precision_score(Y_train,Y_predict_RF, average = 'weighted')\n",
    "    average_metric_RF = (accuracy_RF + recall_RF + f1_RF)/3\n",
    "\n",
    "    # 5. Try an ensemble of all the other classifiers (not RF) using the voting classifier method\n",
    "    ensemble_clf = VotingClassifier(\n",
    "            estimators = [('NB',clf_NB),('KNN',clf_KNN),('svm',clf_svm),('RF',clf_RF)],\n",
    "            voting = 'hard')\n",
    "    ensemble_clf.fit(X_train,Y_train)\n",
    "    accuracy_ensemble = ensemble_clf.score(X_train,Y_train)\n",
    "    Y_predict_ensemble = ensemble_clf.predict(X_train)\n",
    "    conf_mx_ensemble = confusion_matrix(Y_train,Y_predict_ensemble)\n",
    "    recall_ensemble = recall_score(Y_train,Y_predict_ensemble,average=\"weighted\")\n",
    "    f1_ensemble = f1_score(Y_train, Y_predict_ensemble, average=\"weighted\")\n",
    "    precision_ensemble = precision_score(Y_train,Y_predict_ensemble, average = 'weighted')\n",
    "    average_metric_ensemble = (accuracy_ensemble + recall_ensemble + f1_ensemble)/3\n",
    "    \n",
    "    print()\n",
    "    print('*** MODEL TEST SUMMARY ***')\n",
    "    print('KNN accuracy = ',accuracy_KNN, 'KNN_F1_Score = ', f1_KNN, 'KNN Recall = ', recall_KNN, 'KNN precision = ', precision_KNN)\n",
    "    print('Naive Bayes accuracy = ', accuracy_NB, 'Naive_Bayes_F1_Score = ',f1_NB, 'Naive Bayes Recall = ',recall_NB, 'Naive Bayes Precision = ', precision_NB)\n",
    "    print('SVM accuracy = ',accuracy_svm, 'SVM_F1_Score = ', f1_svm, 'SVM recall = ', recall_svm, 'SVM Precision = ', precision_svm)\n",
    "    print('Random Forest accuracy',accuracy_RF,'Random Forest F1 Score = ', f1_RF, 'Random Forest Recall', recall_RF, 'Random Forest Precision = ', precision_RF)    \n",
    "    print('Ensemble accuracy',accuracy_ensemble,'Ensemble F1 Score = ', f1_ensemble, 'Ensemble Recall', recall_ensemble, 'Ensemble Precision = ', precision_ensemble)\n",
    "    \n",
    "    # PLOT CONFUSION MATRICES\n",
    "    if plot_all_conf_mx:\n",
    "        \n",
    "        plt.figure()    \n",
    "        plt.imshow(conf_mx_NB)\n",
    "        plt.title('NB Model Confusion matrix')\n",
    "        plt.colorbar()\n",
    "        classes = clf_NB.classes_\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(conf_mx_KNN)\n",
    "        plt.title('KNN Model Confusion matrix')\n",
    "        plt.colorbar()\n",
    "        classes = clf_KNN.classes_\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(conf_mx_svm)\n",
    "        plt.title('SVM Model Confusion matrix')\n",
    "        plt.colorbar()\n",
    "        classes = clf_svm.classes_\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "    \n",
    "        plt.figure()\n",
    "        plt.imshow(conf_mx_RF)\n",
    "        plt.title('Random Forest Model Confusion matrix')\n",
    "        plt.colorbar()\n",
    "        classes = clf_RF.classes_\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(conf_mx_ensemble)\n",
    "        plt.title('Ensemble Model Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        classes = ensemble_clf.classes_\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        \n",
    "    print() #line break\n",
    "    \n",
    "    if error_selector == 'accuracy':\n",
    "        \n",
    "        if accuracy_KNN > accuracy_svm and accuracy_KNN > accuracy_NB and accuracy_KNN > accuracy_RF and accuracy_KNN > accuracy_ensemble:\n",
    "            clf = neighbours.KNeighboursClassifier()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('KNN model chosen')\n",
    "        \n",
    "        elif accuracy_NB > accuracy_KNN and accuracy_NB > accuracy_svm and accuracy_NB > accuracy_RF and accuracy_NB > accuracy_ensemble:\n",
    "            clf = clf_NB\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Naive Bayes model chosen')\n",
    "            \n",
    "        elif accuracy_svm > accuracy_NB and accuracy_svm > accuracy_KNN and accuracy_KNN > accuracy_RF and accuracy_svm > accuracy_ensemble:\n",
    "            clf = clf_svm\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('SVM model chosen')\n",
    "            print('SVM Params: C = ', C, ' gamma = ', gamma, ' kernel = ', kernel)\n",
    "\n",
    "        elif accuracy_RF > accuracy_NB and accuracy_RF > accuracy_KNN and accuracy_RF > accuracy_ensemble and accuracy_RF> accuracy_svm:\n",
    "            clf = clf_RF\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('RF model chosen')\n",
    "            \n",
    "        elif accuracy_ensemble > accuracy_svm and accuracy_ensemble > accuracy_NB and accuracy_ensemble > accuracy_RF and accuracy_ensemble > accuracy_KNN:\n",
    "            clf = clf_ensemble\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Ensemble model chosen')\n",
    "\n",
    "\n",
    "    elif error_selector == 'recall':\n",
    "\n",
    "\n",
    "        if recall_KNN > recall_svm and recall_KNN > recall_NB and recall_KNN > recall_RF and recall_KNN > recall_ensemble:\n",
    "            clf = neighbours.KNeighboursClassifier()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('KNN model chosen')\n",
    "        \n",
    "        elif recall_NB > recall_KNN and recall_NB > recall_svm and recall_NB > recall_RF and recall_NB > recall_ensemble:\n",
    "            clf = GaussianNB()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Naive Bayes model chosen')\n",
    "            \n",
    "        elif recall_svm > recall_NB and recall_svm > recall_KNN and recall_svm > recall_RF and recall_svm > recall_ensemble:\n",
    "            clf = svm.SVC(kernel=kernel, C=C, gamma = gamma, probability=True)\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('SVM model chosen')\n",
    "            print('SVM Params: C = ', C, ' gamma = ', gamma, ' kernel = ', kernel)\n",
    "        \n",
    "        elif recall_RF > recall_NB and recall_RF > recall_KNN and recall_RF > recall_ensemble and recall_RF> recall_svm:\n",
    "            clf = clf_RF\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('RF model chosen')\n",
    "\n",
    "\n",
    "        elif recall_ensemble > recall_svm and recall_ensemble > recall_NB and recall_NB > recall_RF and recall_ensemble > recall_KNN:\n",
    "            clf = VotingClassifier(\n",
    "                    estimators = [('NB',clf_NB),('SVM',clf_svm),('KNN',clf_KNN)], voting = 'hard')\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Ensemble model chosen')\n",
    "\n",
    "        \n",
    "    elif error_selector == 'F1':\n",
    "        \n",
    "        if f1_KNN > f1_svm and f1_KNN > f1_NB and f1_KNN > f1_RF and f1_KNN > f1_ensemble:\n",
    "            clf = neighbours.KNeighboursClassifier()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('KNN model chosen')\n",
    "        \n",
    "        elif f1_NB > f1_KNN and f1_NB > f1_svm and f1_NB > f1_RF and f1_NB > f1_ensemble:\n",
    "            clf = GaussianNB()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Naive Bayes model chosen')\n",
    "            \n",
    "        elif f1_svm > f1_NB and f1_svm > f1_KNN and f1_svm > f1_RF and f1_svm > f1_ensemble:\n",
    "            clf = svm.SVC(kernel=kernel, C=C, gamma = gamma, probability = True)\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('SVM model chosen')\n",
    "            print('SVM Params: C = ', C, ' gamma = ', gamma, ' kernel = ', kernel)\n",
    "            \n",
    "        elif f1_RF > f1_NB and f1_RF > f1_KNN and f1_RF > f1_ensemble and f1_RF> f1_svm:\n",
    "            clf = clf_RF\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('RF model chosen')\n",
    "        \n",
    "        elif f1_ensemble > f1_svm and f1_ensemble > f1_NB and f1_ensemble > f1_RF and f1_ensemble > f1_KNN:\n",
    "            clf = VotingClassifier(\n",
    "                    estimators = [('NB',clf_NB),('SVM',clf_svm),('KNN',clf_KNN)], voting = 'hard')\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Ensemble model chosen')\n",
    "            \n",
    "\n",
    "    elif error_selector == 'precision':\n",
    "        \n",
    "        if precision_KNN > precision_svm and precision_KNN > precision_NB and precision_KNN > precision_RF and precision_KNN > precision_ensemble:\n",
    "            clf = neighbours.KNeighboursClassifier()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('KNN model chosen')\n",
    "        \n",
    "        elif precision_NB > precision_KNN and precision_NB > precision_svm and precision_NB > precision_RF and precision_NB > precision_ensemble:\n",
    "            clf = GaussianNB()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Naive Bayes model chosen')\n",
    "            \n",
    "        elif precision_RF > precision_NB and precision_RF > precision_KNN and precision_RF > precision_ensemble and precision_RF> precision_svm:\n",
    "            clf = clf_RF\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('RF model chosen')\n",
    "            \n",
    "        elif precision_svm > precision_NB and precision_svm > precision_KNN and precision_svm > precision_RF and precision_svm > precision_ensemble:\n",
    "            clf = svm.SVC(kernel=kernel, C=C, gamma = gamma, probability=True)\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('SVM model chosen')\n",
    "            print('SVM Params: C = ', C, ' gamma = ', gamma, ' kernel = ', kernel)\n",
    "        \n",
    "        elif precision_ensemble > precision_svm and precision_ensemble > precision_NB and precision_ensemble > precision_RF and precision_ensemble > precision_KNN:\n",
    "            clf = VotingClassifier(\n",
    "                    estimators = [('NB',clf_NB),('SVM',clf_svm),('KNN',clf_KNN)], voting = 'hard')\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Ensemble model chosen')\n",
    "\n",
    "    elif error_selector == 'average_all_metric':\n",
    "        \n",
    "        if average_metric_KNN > average_metric_svm and average_metric_KNN > average_metric_NB and average_metric_KNN > average_metric_RF and average_metric_KNN > average_metric_ensemble:\n",
    "            clf = neighbours.KNeighboursClassifier()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('KNN model chosen')\n",
    "        \n",
    "        elif average_metric_NB > average_metric_KNN and average_metric_NB > average_metric_svm and average_metric_NB > average_metric_RF and average_metric_NB > average_metric_ensemble:\n",
    "            clf = GaussianNB()\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Naive Bayes model chosen')\n",
    "            \n",
    "        elif average_metric_RF > average_metric_NB and average_metric_RF > average_metric_KNN and average_metric_RF > average_metric_ensemble and average_metric_RF> average_metric_svm:\n",
    "            clf = clf_RF\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('RF model chosen')\n",
    "            \n",
    "        elif average_metric_svm > average_metric_NB and average_metric_svm > average_metric_KNN and average_metric_svm > average_metric_RF and average_metric_svm > average_metric_ensemble:\n",
    "            clf = svm.SVC(kernel=kernel, C=C, gamma = gamma, probability=True)\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('SVM model chosen')\n",
    "            print('SVM Params: C = ', C, ' gamma = ', gamma, ' kernel = ', kernel)\n",
    "        \n",
    "        elif average_metric_ensemble > average_metric_svm and average_metric_ensemble > average_metric_NB and average_metric_ensemble > average_metric_RF and average_metric_ensemble > average_metric_KNN:\n",
    "            clf = VotingClassifier(\n",
    "                    estimators = [('NB',clf_NB),('SVM',clf_svm),('KNN',clf_KNN)], voting = 'hard')\n",
    "            clf.fit(X_train,Y_train)\n",
    "            print('Ensemble model chosen')\n",
    "            \n",
    "# Now that model has been selected using error metrics from training data, the final\n",
    "# model can be evaluated on the test set. The code below therefore measures the f1, recall,\n",
    "# confusion matrix and accuracy  for the final selected model and prints to ipython.\n",
    "            \n",
    "    Y_test_predicted = clf.predict(X_test)\n",
    "    final_conf_mx = confusion_matrix(Y_test, Y_test_predicted)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(final_conf_mx)\n",
    "    plt.title('Final Model Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    classes = clf.classes_\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Normalise confusion matrix to show errors\n",
    "    row_sums = final_conf_mx.sum(axis=1, keepdims=True)\n",
    "    norm_conf_mx = final_conf_mx / row_sums\n",
    "    np.fill_diagonal(norm_conf_mx, 0)\n",
    "    plt.figure()\n",
    "    plt.imshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "    plt.colorbar()\n",
    "    plt.clim(0,1)\n",
    "    plt.title('Normalised Confusion Matrix')\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    final_recall = recall_score(Y_test,Y_test_predicted,average=\"weighted\")\n",
    "    final_f1 = f1_score(Y_test, Y_test_predicted, average=\"weighted\")\n",
    "    final_accuracy = clf.score(X_test,Y_test)\n",
    "    final_precision = precision_score(Y_test, Y_test_predicted, average='weighted')\n",
    "    final_average_metric = (final_recall + final_accuracy + final_f1)/3\n",
    "\n",
    "    # The Feature importances \n",
    "    print()\n",
    "    print('Feature Importances')\n",
    "    print('(relative importance of each feature (wavelength) for prediction)')\n",
    "    print()\n",
    "    for name, score in zip(X.columns,clf.feature_importances_):\n",
    "        print (name,score)\n",
    "\n",
    "    print() #line break\n",
    "    print ('*** FINAL MODEL SUMMARY ***')\n",
    "    print('Final Model Accuracy = ', final_accuracy)\n",
    "    print('Final Model Recall = ', final_recall)\n",
    "    print('Final Model F1 = ', final_f1)\n",
    "    print('Final Model Precision = ',final_precision)\n",
    "    print('Final Model Average metric = ', final_average_metric)\n",
    "\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call function\n",
    "# NB UndefinedMetricWarning is informing user that the performance estimates are only meaningful if several\n",
    "# instances from each surface class are available in both the training and test sets. Rerun the cell until this condition\n",
    "# is satisifed (no warning shown)\n",
    "\n",
    "clf =  optimise_train_model(X,XX,YY, error_selector = 'precision', test_size = 0.3, plot_all_conf_mx = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the model to file so that it can be re-used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_classifier(clf):\n",
    "    \n",
    "    # pickle the classifier model for archiving or for reusing in another code\n",
    "    joblibfile = 'Sentinel2_classifier.pkl'\n",
    "    joblib.dump(clf,joblibfile)\n",
    "    \n",
    "    # to load this classifier into another code use the following syntax:\n",
    "    #clf = joblib.load(joblib_file)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call function\n",
    "#save_classifier(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classify images using the trained algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ClassifyImages(Sentinel_jp2s,clf, savefigs=False):\n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    # Import multispectral imagery from Sentinel 2 and apply ML algorithm to classify surface\n",
    "    \n",
    "    jp2s = Seninel_jp2s\n",
    "    arrs = []   \n",
    "    for jp2 in jp2s:\n",
    "        with rasterio.open(jp2) as f:\n",
    "            arrs.append(f.read(1))\n",
    "    \n",
    "    data = np.array(arrs, dtype=arrs[0].dtype)\n",
    "\n",
    "    # set up empty lists to append into\n",
    "    B2 = []\n",
    "    B3 = []\n",
    "    B4 = []\n",
    "    B5 = []\n",
    "    B6 = []\n",
    "    B7 = []\n",
    "    B8 =[]\n",
    "    B11 = []\n",
    "    B12 = []\n",
    "    \n",
    "    predicted = []\n",
    "    test_array = []\n",
    "    albedo_array = []\n",
    "    # get dimensions of each band layer\n",
    "    lenx, leny = np.shape(data[0])\n",
    "    \n",
    "    #convert image bands into single 5-dimensional numpy array\n",
    "    test_array = np.array([data[0]/10000, data[1]/10000,data[2]/10000,data[3]/10000,data[4]/10000,\n",
    "                          data[5]/10000,data[6]/10000,data[7]/10000,data[8]/10000])       \n",
    "    test_array = test_array.reshape(9,lenx*leny) #reshape into 5 x 1D arrays\n",
    "    test_array = test_array.transpose() # transpose sot hat bands are read as features\n",
    "    # create albedo array by applying Knap (1999) narrowband - broadband conversion\n",
    "    albedo_array = np.array([0.356*(data[0]/10000)+0.13*(data[2]/10000)+0.373*(data[6]/10000)+0.085*(data[7]/10000)+0.072*(data[8]/10000)-0.0018])\n",
    "\n",
    "    #apply classifier to each pixel in multispectral image with bands as features   \n",
    "    predicted = clf.predict(test_array)\n",
    "\n",
    "    # apply ML algorithm to 4-value array for each pixel - predict surface type    \n",
    "    predicted = clf.predict(test_array)\n",
    "    \n",
    "    # convert surface class (string) to a numeric value for plotting\n",
    "    predicted[predicted == 'SN'] = float(1)\n",
    "    predicted[predicted == 'WAT'] = float(2)\n",
    "    predicted[predicted == 'CC'] = float(3)\n",
    "    predicted[predicted == 'CI'] = float(4)\n",
    "    predicted[predicted == 'LA'] = float(5)\n",
    "    predicted[predicted == 'HA'] = float(6)\n",
    "    \n",
    "    # ensure array data type is float (required for imshow)\n",
    "    predicted = predicted.astype(float)\n",
    "    \n",
    "    # reshape 1D array back into original image dimensions\n",
    "    predicted = np.reshape(predicted,[lenx,leny])\n",
    "    albedo_array = np.reshape(albedo_array,[lenx,leny])\n",
    "\n",
    "    #split image into 3 ice-covered areas that together represent majority of glaciated land in image  \n",
    "          \n",
    "    predicted1 = predicted[2170:2975,2130:4350]\n",
    "    predicted2 = predicted[1400:2160,2630:4740]\n",
    "    predicted3 = predicted[0:1405,2800:5075]\n",
    "    albedo_array1 = albedo_array[2170:2975,2130:4350] \n",
    "    albedo_array2 = albedo_array[1400:2160,2630:4740]\n",
    "    albedo_array3 = albedo_array[0:1405,2800:5075]\n",
    "\n",
    "    cmap1 = mpl.colors.ListedColormap(['white','slategray','black','lightsteelblue','gold','orangered'])\n",
    "    cmap2 = 'Greys_r'\n",
    "    \n",
    "    #plot classified surface\n",
    "    plt.figure(figsize = (30,9)),plt.imshow(predicted1,cmap=cmap1),plt.colorbar(cmap=cmap1),plt.grid(None)\n",
    "    if savefigs:\n",
    "        plt.savefig('2016Clasified_Sentinel_20m_Area1.png',dpi=300)\n",
    "    \n",
    "    plt.figure(figsize = (30,8)),plt.imshow(predicted2,cmap=cmap1),plt.colorbar(cmap=cmap1),plt.grid(None)\n",
    "    if savefigs:\n",
    "        plt.savefig('2016Clasified_Sentinel_20m_Area2.png',dpi=300)\n",
    "    \n",
    "    plt.figure(figsize = (30,8)),plt.imshow(predicted3,cmap=cmap1),plt.colorbar(cmap=cmap1),plt.grid(None)\n",
    "    if savefigs:\n",
    "        plt.savefig('2016Clasified_Sentinel_20m_Area3.png',dpi=300)\n",
    "\n",
    "    plt.figure(figsize = (30,9)),plt.imshow(albedo_array1,cmap=cmap2,vmin=0,vmax=1),plt.colorbar(cmap=cmap2),plt.grid(None)\n",
    "    if savefigs:\n",
    "        plt.savefig('2016Albedo_Sentinel_20m_Area1.png',dpi=300)\n",
    "    \n",
    "    plt.figure(figsize = (30,8)),plt.imshow(albedo_array2,cmap=cmap2,vmin=0,vmax=1),plt.colorbar(cmap=cmap2),plt.grid(None)\n",
    "    if savefigs:\n",
    "        plt.savefig('2016Albedo_Sentinel_20m_Area2.png',dpi=300)\n",
    "    \n",
    "    plt.figure(figsize = (30,8)),plt.imshow(albedo_array3,cmap=cmap2,vmin=0,vmax=1),plt.colorbar(cmap=cmap2),plt.grid(None)\n",
    "    if savefigs:\n",
    "        plt.savefig('2016Albedo_Sentinel_20m_Area3.png',dpi=300)\n",
    "    \n",
    "    print()\n",
    "    print('Time taken to classify image = ',datetime.now() - startTime)\n",
    "    print('Below are classified images of each of three ablation zone areas along with albedo maps for the same areas')\n",
    "\n",
    "    return predicted1, predicted2, predicted3,albedo_array1,albedo_array2,albedo_array3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Function\n",
    "# WARNING: This section takes ~15 minutes to run (w/ 32GB RAM, i7-7700Ghz processor)\n",
    "\n",
    "predicted1,predicted2,predicted3,albedo_array1,albedo_array2,albedo_array3 =  ClassifyImages(Seninel_jp2s,clf,savefigs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### analyse the classified image and report the coverage statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CoverageStats(predicted1, predicted2, predicted3):\n",
    "    res = 0.02 # Ground resolution of sentinel data in km\n",
    "    counter = 0\n",
    "    \n",
    "    for i in predicted1,predicted2,predicted3:\n",
    "        \n",
    "        counter +=1\n",
    "\n",
    "        x,y = np.shape(i)\n",
    "        area_of_image = (x*res)*(y*res) # area of selected region\n",
    "\n",
    "        # Calculate coverage stats\n",
    "        numHA = (i==6).sum()\n",
    "        numLA = (i==5).sum()\n",
    "        numCI = (i==4).sum()\n",
    "        numCC = (i==3).sum()\n",
    "        numWAT = (i==2).sum()\n",
    "        numSN = (i==1).sum()\n",
    "        total_pix = numHA+numLA+numCI+numCC+numWAT+numSN\n",
    "        \n",
    "        tot_alg_coverage = (numHA+numLA)/total_pix*100\n",
    "        HA_coverage = (numHA)/total_pix * 100\n",
    "        LA_coverage = (numLA)/total_pix * 100\n",
    "        CI_coverage = (numCI)/total_pix * 100\n",
    "        CC_coverage = (numCC)/total_pix * 100\n",
    "        WAT_coverage = (numWAT)/total_pix * 100\n",
    "        SN_coverage = (numSN)/total_pix * 100\n",
    "        \n",
    "        # Print coverage summary\n",
    "        print()\n",
    "        print('**** SUMMARY ****')\n",
    "        print('{} Area of image = '.format(counter), area_of_image, 'km')\n",
    "        print('{}  % algal coverage (Hbio + Lbio) = '.format(counter),np.round(tot_alg_coverage,2))\n",
    "        print('{}  % Hbio coverage = '.format(counter),np.round(HA_coverage,2))\n",
    "        print('{}  % Lbio coverage = '.format(counter),np.round(LA_coverage,2))\n",
    "        print('{}  % cryoconite coverage = '.format(counter),np.round(CC_coverage,2))\n",
    "        print('{}  % clean ice coverage = '.format(counter),np.round(CI_coverage,2))\n",
    "        print('{}  % water coverage = '.format(counter),np.round(WAT_coverage,2))\n",
    "        print('{}  % snow coverage = '.format(counter),np.round(SN_coverage,2))\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function\n",
    "CoverageStats(predicted1,predicted2,predicted3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Report the albedo statistics for each of the three sub-areas within the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def albedo_report_by_site(predicted1,predicted2,predicted3,albedo_array1,albedo_array2,albedo_array3):\n",
    "        \n",
    "    counter = 0\n",
    "    \n",
    "    albedo_DF1 = pd.DataFrame()\n",
    "    albedo_DF2 = pd.DataFrame()\n",
    "    albedo_DF3 = pd.DataFrame()\n",
    "    HA_DF1 = pd.DataFrame()\n",
    "    LA_DF1 = pd.DataFrame()\n",
    "    WAT_DF1 = pd.DataFrame()\n",
    "    CI_DF1 = pd.DataFrame()\n",
    "    CC_DF1 = pd.DataFrame()\n",
    "    SN_DF1 = pd.DataFrame()\n",
    "    HA_DF2 = pd.DataFrame()\n",
    "    LA_DF2 = pd.DataFrame()\n",
    "    WAT_DF2 = pd.DataFrame()\n",
    "    CI_DF2 = pd.DataFrame()\n",
    "    CC_DF2 = pd.DataFrame()\n",
    "    SN_DF2 = pd.DataFrame()\n",
    "    HA_DF3 = pd.DataFrame()\n",
    "    LA_DF3 = pd.DataFrame()\n",
    "    WAT_DF3 = pd.DataFrame()\n",
    "    CI_DF3 = pd.DataFrame()\n",
    "    CC_DF3 = pd.DataFrame()\n",
    "    SN_DF3 = pd.DataFrame()\n",
    "    \n",
    "    for i in predicted1,predicted2,predicted3:\n",
    "        \n",
    "        counter +=1\n",
    "        \n",
    "        PP = i.ravel()\n",
    "        print(len(PP))\n",
    "        \n",
    "        if counter == 1:\n",
    "           \n",
    "            AA = albedo_array1.ravel()\n",
    "            albedo_DF1['class'] = PP\n",
    "            albedo_DF1['albedo'] = AA\n",
    "             \n",
    "    # divide albedo dataframe into individual classes for summary stats. include only\n",
    "    # rows where albedo is between 0.05 and 0.95 percentiles to remove outliers\n",
    "        \n",
    "            HA_DF1 = albedo_DF1[albedo_DF1['class'] == 6]\n",
    "            HA_DF1 = HA_DF1[HA_DF1['albedo'] > HA_DF1['albedo'].quantile(0.05)]\n",
    "            HA_DF1 = HA_DF1[HA_DF1['albedo'] < HA_DF1['albedo'].quantile(0.95)]\n",
    "                        \n",
    "            LA_DF1 = albedo_DF1[albedo_DF1['class'] == 5]\n",
    "            LA_DF1 = LA_DF1[LA_DF1['albedo'] > LA_DF1['albedo'].quantile(0.05)]\n",
    "            LA_DF1 = LA_DF1[LA_DF1['albedo'] < LA_DF1['albedo'].quantile(0.95)]     \n",
    "            \n",
    "            CI_DF1 = albedo_DF1[albedo_DF1['class'] == 4]\n",
    "            CI_DF1 = CI_DF1[CI_DF1['albedo'] > CI_DF1['albedo'].quantile(0.05)]\n",
    "            CI_DF1 = CI_DF1[CI_DF1['albedo'] < CI_DF1['albedo'].quantile(0.95)]\n",
    "            \n",
    "            CC_DF1 = albedo_DF1[albedo_DF1['class'] == 3]\n",
    "            CC_DF1 = CC_DF1[CC_DF1['albedo'] > CC_DF1['albedo'].quantile(0.05)]\n",
    "            CC_DF1 = CC_DF1[CC_DF1['albedo'] < CC_DF1['albedo'].quantile(0.95)]\n",
    "        \n",
    "            WAT_DF1 = albedo_DF1[albedo_DF1['class'] == 2]\n",
    "            WAT_DF1 = WAT_DF1[WAT_DF1['albedo'] > WAT_DF1['albedo'].quantile(0.05)]\n",
    "            WAT_DF1 = WAT_DF1[WAT_DF1['albedo'] < WAT_DF1['albedo'].quantile(0.95)]   \n",
    "\n",
    "            SN_DF1 = albedo_DF1[albedo_DF1['class'] == 1]\n",
    "            SN_DF1 = SN_DF1[SN_DF1['albedo'] > SN_DF1['albedo'].quantile(0.05)]\n",
    "            SN_DF1 = SN_DF1[SN_DF1['albedo'] < SN_DF1['albedo'].quantile(0.95)]             \n",
    "           \n",
    "            # Calculate summary stats\n",
    "            mean_CC1 = CC_DF1['albedo'].mean()\n",
    "            std_CC1 = CC_DF1['albedo'].std()\n",
    "            max_CC1 = CC_DF1['albedo'].max()\n",
    "            min_CC1 = CC_DF1['albedo'].min()\n",
    "        \n",
    "            mean_CI1 = CI_DF1['albedo'].mean()\n",
    "            std_CI1 = CI_DF1['albedo'].std()\n",
    "            max_CI1 = CI_DF1['albedo'].max()\n",
    "            min_CI1 = CI_DF1['albedo'].min()\n",
    "            \n",
    "            mean_LA1 = LA_DF1['albedo'].mean()\n",
    "            std_LA1 = LA_DF1['albedo'].std()\n",
    "            max_LA1 = LA_DF1['albedo'].max()\n",
    "            min_LA1 = LA_DF1['albedo'].min()\n",
    "            \n",
    "            mean_HA1 = HA_DF1['albedo'].mean()\n",
    "            std_HA1 = HA_DF1['albedo'].std()\n",
    "            max_HA1 = HA_DF1['albedo'].max()\n",
    "            min_HA1 = HA_DF1['albedo'].min()\n",
    "            \n",
    "            mean_WAT1 = WAT_DF1['albedo'].mean()\n",
    "            std_WAT1 = WAT_DF1['albedo'].std()\n",
    "            max_WAT1 = WAT_DF1['albedo'].max()\n",
    "            min_WAT1 = WAT_DF1['albedo'].min()\n",
    "\n",
    "            mean_SN1 = SN_DF1['albedo'].mean()\n",
    "            std_SN1 = SN_DF1['albedo'].std()\n",
    "            max_SN1 = SN_DF1['albedo'].max()\n",
    "            min_SN1 = SN_DF1['albedo'].min()\n",
    "                \n",
    "    \n",
    "            print()\n",
    "            print('*** Albedo Stats 1 ***')\n",
    "            print()\n",
    "            print('mean albedo WAT 1 = ', mean_WAT1)\n",
    "            print('mean albedo CC 1 = ', mean_CC1)\n",
    "            print('mean albedo CI 1 = ', mean_CI1)\n",
    "            print('mean albedo LA 1 = ', mean_LA1)\n",
    "            print('mean albedo HA 1 = ', mean_HA1)\n",
    "            print('mean albedo SN 1 = ', mean_SN1)\n",
    "            print('n HA 1 = ',len(HA_DF1))\n",
    "            print('n LA 1 = ',len(LA_DF1))\n",
    "            print('n CI 1 = ',len(CI_DF1))\n",
    "            print('n CC 1 = ',len(CC_DF1))\n",
    "            print('n WAT 1 = ',len(WAT_DF1))\n",
    "            print('n SN 1 = ', len(SN_DF1))\n",
    "\n",
    "        elif counter == 2:\n",
    "            \n",
    "            AA = albedo_array2.ravel()\n",
    "            albedo_DF2['class'] = PP\n",
    "            albedo_DF2['albedo'] = AA\n",
    "            \n",
    "            HA_DF2 = albedo_DF2[albedo_DF2['class'] == 6]\n",
    "            HA_DF2 = HA_DF2[HA_DF2['albedo'] > HA_DF2['albedo'].quantile(0.05)]\n",
    "            HA_DF2 = HA_DF2[HA_DF2['albedo'] < HA_DF2['albedo'].quantile(0.95)]\n",
    "                        \n",
    "            LA_DF2 = albedo_DF2[albedo_DF2['class'] == 5]\n",
    "            LA_DF2 = LA_DF2[LA_DF2['albedo'] > LA_DF2['albedo'].quantile(0.05)]\n",
    "            LA_DF2 = LA_DF2[LA_DF2['albedo'] < LA_DF2['albedo'].quantile(0.95)]\n",
    "            \n",
    "            \n",
    "            CI_DF2 = albedo_DF2[albedo_DF2['class'] == 4]\n",
    "            CI_DF2 = CI_DF2[CI_DF2['albedo'] > CI_DF2['albedo'].quantile(0.05)]\n",
    "            CI_DF2 = CI_DF2[CI_DF2['albedo'] < CI_DF2['albedo'].quantile(0.95)]\n",
    "            \n",
    "        \n",
    "            CC_DF2 = albedo_DF2[albedo_DF2['class'] == 3]\n",
    "            CC_DF2 = CC_DF2[CC_DF2['albedo'] > CC_DF2['albedo'].quantile(0.05)]\n",
    "            CC_DF2 = CC_DF2[CC_DF2['albedo'] < CC_DF2['albedo'].quantile(0.95)]\n",
    "            \n",
    "        \n",
    "            WAT_DF2 = albedo_DF2[albedo_DF2['class'] == 2]\n",
    "            WAT_DF2 = WAT_DF2[WAT_DF2['albedo'] > WAT_DF2['albedo'].quantile(0.05)]\n",
    "            WAT_DF2 = WAT_DF2[WAT_DF2['albedo'] < WAT_DF2['albedo'].quantile(0.95)]   \n",
    "                \n",
    "            SN_DF2 = albedo_DF2[albedo_DF2['class'] == 1]\n",
    "            SN_DF2 = SN_DF2[SN_DF2['albedo'] > SN_DF2['albedo'].quantile(0.05)]\n",
    "            SN_DF2 = SN_DF2[SN_DF2['albedo'] < SN_DF2['albedo'].quantile(0.95)]               \n",
    "           \n",
    "            # Calculate summary stats\n",
    "            mean_CC2 = CC_DF2['albedo'].mean()\n",
    "            std_CC2 = CC_DF2['albedo'].std()\n",
    "            max_CC2 = CC_DF2['albedo'].max()\n",
    "            min_CC2 = CC_DF2['albedo'].min()\n",
    "        \n",
    "            mean_CI2 = CI_DF2['albedo'].mean()\n",
    "            std_CI2 = CI_DF2['albedo'].std()\n",
    "            max_CI2 = CI_DF2['albedo'].max()\n",
    "            min_CI2 = CI_DF2['albedo'].min()\n",
    "            \n",
    "            mean_LA2 = LA_DF2['albedo'].mean()\n",
    "            std_LA2 = LA_DF2['albedo'].std()\n",
    "            max_LA2 = LA_DF2['albedo'].max()\n",
    "            min_LA2 = LA_DF2['albedo'].min()\n",
    "            \n",
    "            mean_HA2 = HA_DF2['albedo'].mean()\n",
    "            std_HA2 = HA_DF2['albedo'].std()\n",
    "            max_HA2 = HA_DF2['albedo'].max()\n",
    "            min_HA2 = HA_DF2['albedo'].min()\n",
    "            \n",
    "            mean_WAT2 = WAT_DF2['albedo'].mean()\n",
    "            std_WAT2 = WAT_DF2['albedo'].std()\n",
    "            max_WAT2 = WAT_DF2['albedo'].max()\n",
    "            min_WAT2 = WAT_DF2['albedo'].min()\n",
    "\n",
    "            mean_SN2 = SN_DF2['albedo'].mean()\n",
    "            std_SN2 = SN_DF2['albedo'].std()\n",
    "            max_SN2 = SN_DF2['albedo'].max()\n",
    "            min_SN2 = SN_DF2['albedo'].min()\n",
    "            \n",
    "            print()\n",
    "            print('*** Albedo Stats 2 ***')\n",
    "            print()\n",
    "            print('mean albedo WAT 2 = ', mean_WAT2)\n",
    "            print('mean albedo CC 2 = ', mean_CC2)\n",
    "            print('mean albedo CI 2 = ', mean_CI2)\n",
    "            print('mean albedo LA 2 = ', mean_LA2)\n",
    "            print('mean albedo HA 2 = ', mean_HA2)\n",
    "            print('mean albedo SN 2 = ', mean_SN2)\n",
    "            print('n HA 2 = ',len(HA_DF2))\n",
    "            print('n LA 2 = ',len(LA_DF2))\n",
    "            print('n CI 2 = ',len(CI_DF2))\n",
    "            print('n CC 2 = ',len(CC_DF2))\n",
    "            print('n WAT 2 = ',len(WAT_DF2))\n",
    "            print('n SN 2 = ',len(SN_DF2))\n",
    "                \n",
    "        elif counter == 3:\n",
    "            \n",
    "            AA = albedo_array3.ravel()\n",
    "            albedo_DF3['class'] = PP\n",
    "            albedo_DF3['albedo'] = AA    \n",
    "            \n",
    "            HA_DF3 = albedo_DF3[albedo_DF3['class'] == 6]\n",
    "            HA_DF3 = HA_DF3[HA_DF3['albedo'] > HA_DF3['albedo'].quantile(0.05)]\n",
    "            HA_DF3 = HA_DF3[HA_DF3['albedo'] < HA_DF3['albedo'].quantile(0.95)]\n",
    "                        \n",
    "            LA_DF3 = albedo_DF3[albedo_DF3['class'] == 5]\n",
    "            LA_DF3 = LA_DF3[LA_DF3['albedo'] > LA_DF3['albedo'].quantile(0.05)]\n",
    "            LA_DF3 = LA_DF3[LA_DF3['albedo'] < LA_DF3['albedo'].quantile(0.95)]\n",
    "            \n",
    "            CI_DF3 = albedo_DF3[albedo_DF3['class'] == 4]\n",
    "            CI_DF3 = CI_DF3[CI_DF3['albedo'] > CI_DF3['albedo'].quantile(0.05)]\n",
    "            CI_DF3 = CI_DF3[CI_DF3['albedo'] < CI_DF3['albedo'].quantile(0.95)]\n",
    "            \n",
    "            CC_DF3 = albedo_DF3[albedo_DF3['class'] == 3]\n",
    "            CC_DF3 = CC_DF3[CC_DF3['albedo'] > CC_DF3['albedo'].quantile(0.05)]\n",
    "            CC_DF3 = CC_DF3[CC_DF3['albedo'] < CC_DF3['albedo'].quantile(0.95)]\n",
    "            \n",
    "            WAT_DF3 = albedo_DF3[albedo_DF3['class'] == 2]\n",
    "            WAT_DF3 = WAT_DF3[WAT_DF3['albedo'] > WAT_DF3['albedo'].quantile(0.05)]\n",
    "            WAT_DF3 = WAT_DF3[WAT_DF3['albedo'] < WAT_DF3['albedo'].quantile(0.95)]   \n",
    "\n",
    "            SN_DF3 = albedo_DF3[albedo_DF3['class'] == 1]\n",
    "            SN_DF3 = SN_DF3[SN_DF3['albedo'] > SN_DF3['albedo'].quantile(0.05)]\n",
    "            SN_DF3 = SN_DF3[SN_DF3['albedo'] < SN_DF3['albedo'].quantile(0.95)]              \n",
    "           \n",
    "            # Calculate summary stats\n",
    "            mean_CC3 = CC_DF3['albedo'].mean()\n",
    "            std_CC3 = CC_DF3['albedo'].std()\n",
    "            max_CC3 = CC_DF3['albedo'].max()\n",
    "            min_CC3 = CC_DF3['albedo'].min()\n",
    "        \n",
    "            mean_CI3 = CI_DF3['albedo'].mean()\n",
    "            std_CI3 = CI_DF3['albedo'].std()\n",
    "            max_CI3 = CI_DF3['albedo'].max()\n",
    "            min_CI3 = CI_DF3['albedo'].min()\n",
    "            \n",
    "            mean_LA3 = LA_DF3['albedo'].mean()\n",
    "            std_LA3 = LA_DF3['albedo'].std()\n",
    "            max_LA3 = LA_DF3['albedo'].max()\n",
    "            min_LA3 = LA_DF3['albedo'].min()\n",
    "            \n",
    "            mean_HA3 = HA_DF3['albedo'].mean()\n",
    "            std_HA3 = HA_DF3['albedo'].std()\n",
    "            max_HA3 = HA_DF3['albedo'].max()\n",
    "            min_HA3 = HA_DF3['albedo'].min()\n",
    "            \n",
    "            mean_WAT3 = WAT_DF3['albedo'].mean()\n",
    "            std_WAT3 = WAT_DF3['albedo'].std()\n",
    "            max_WAT3 = WAT_DF3['albedo'].max()\n",
    "            min_WAT3 = WAT_DF3['albedo'].min()\n",
    "            \n",
    "            mean_SN3 = SN_DF3['albedo'].mean()\n",
    "            std_SN3 = SN_DF3['albedo'].std()\n",
    "            max_SN3 = SN_DF3['albedo'].max()\n",
    "            min_SN3 = SN_DF3['albedo'].min()\n",
    "        \n",
    "            print()\n",
    "            print('*** Albedo Stats 3 ***')\n",
    "            print()\n",
    "            print('mean albedo WAT 3 = ', mean_WAT3)\n",
    "            print('mean albedo CC 3 = ', mean_CC3)\n",
    "            print('mean albedo CI 3 = ', mean_CI3)\n",
    "            print('mean albedo LA 3 = ', mean_LA3)\n",
    "            print('mean albedo HA 3 = ', mean_HA3)\n",
    "            print('mean albedo SN 3 = ', mean_SN3)\n",
    "            print('n HA 3 = ',len(HA_DF3))\n",
    "            print('n LA 3 = ',len(LA_DF3))\n",
    "            print('n CI 3 = ',len(CI_DF3))\n",
    "            print('n CC 3 = ',len(CC_DF3))\n",
    "            print('n WAT 3 = ',len(WAT_DF3))\n",
    "            print('n SN 3 = ',len(SN_DF3))\n",
    "            \n",
    "        #albedo_DF1.to_csv('2016Sentinel_20m_albedo_dataset_Area1.csv')    \n",
    "        #albedo_DF2.to_csv('2016Sentinel_20m_albedo_dataset_Area2.csv')\n",
    "        #albedo_DF3.to_csv('2016Sentinel_20m_albedo_dataset_Area3.csv')\n",
    "        albedo_DFall = pd.concat([albedo_DF1,albedo_DF2,albedo_DF3])\n",
    "        #albedo_DFall.to_csv('2016Sentinel_20m_albedo_dataset_allsites.csv')\n",
    "        \n",
    "    return albedo_DF1,albedo_DF2,albedo_DF3,albedo_DFall,HA_DF1,LA_DF1,CI_DF1,CC_DF1,WAT_DF1,SN_DF1,HA_DF2,LA_DF2,CI_DF2,CC_DF2,WAT_DF2,SN_DF2,HA_DF3,LA_DF3,CI_DF3,CC_DF3,WAT_DF3,SN_DF3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function\n",
    "albedo_DF1,albedo_DF2,albedo_DF3,albedoDFall,HA_DF1,LA_DF1,CI_DF1,CC_DF1,WAT_DF1,SN_DF1,HA_DF2,LA_DF2,CI_DF2,CC_DF2,WAT_DF2,SN_DF2,HA_DF3,LA_DF3,CI_DF3,CC_DF3,WAT_DF3,SN_DF3 = albedo_report_by_site(predicted1,predicted2,predicted3,albedo_array1,albedo_array2,albedo_array3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return the summary statistics over the three sub-areas combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def albedo_report_all_sites(albedo_DFall,HA_DF1,LA_DF1,CI_DF1,CC_DF1,WAT_DF1,SN_DF1,HA_DF2,LA_DF2,CI_DF2,CC_DF2,WAT_DF2,SN_DF2,HA_DF3,LA_DF3,CI_DF3,CC_DF3,WAT_DF3,SN_DF3):\n",
    "    \n",
    "    HA_DF = pd.concat([HA_DF1,HA_DF2,HA_DF3])\n",
    "    LA_DF = pd.concat([LA_DF1,LA_DF2,LA_DF3])\n",
    "    CI_DF = pd.concat([CI_DF1,CI_DF2,CI_DF3])\n",
    "    CC_DF = pd.concat([CC_DF1,CC_DF2,CC_DF3])\n",
    "    WAT_DF = pd.concat([WAT_DF1,WAT_DF2,WAT_DF3])\n",
    "    SN_DF = pd.concat([SN_DF1,SN_DF2,SN_DF3])\n",
    "    \n",
    "    print('**SUMMARY FOR ALL SITES COMBINED ***')\n",
    "    print()\n",
    "    print('Mean Albedo HA: ',HA_DF['albedo'].mean())\n",
    "    print('Std Albedo HA: ',HA_DF['albedo'].std())\n",
    "    print('Min Albedo HA: ',HA_DF['albedo'].min())\n",
    "    print('Max Albedo HA: ',HA_DF['albedo'].max())\n",
    "    print('HA number of observations (n) = ', len(HA_DF))\n",
    "    print()\n",
    "    print('Mean Albedo LA: ',LA_DF['albedo'].mean())\n",
    "    print('Std Albedo LA: ',LA_DF['albedo'].std())\n",
    "    print('Min Albedo LA: ',LA_DF['albedo'].min())\n",
    "    print('Max Albedo LA: ',LA_DF['albedo'].max())\n",
    "    print('LA number of observations (n) = ', len(LA_DF))\n",
    "    print()    \n",
    "    print('Mean Albedo CI: ',CI_DF['albedo'].mean())\n",
    "    print('Std Albedo CI: ',CI_DF['albedo'].std())\n",
    "    print('Min Albedo CI: ',CI_DF['albedo'].min())\n",
    "    print('Max Albedo CI: ',CI_DF['albedo'].max())\n",
    "    print('CI number of observations (n) = ', len(CI_DF))\n",
    "    print()\n",
    "    print('Mean Albedo CC: ',CC_DF['albedo'].mean())\n",
    "    print('Std Albedo CC: ',CC_DF['albedo'].std())\n",
    "    print('Min Albedo CC: ',CC_DF['albedo'].min())\n",
    "    print('Max Albedo CC: ',CC_DF['albedo'].max())\n",
    "    print('CC number of observations (n) = ', len(CC_DF))\n",
    "    print()\n",
    "    print('Mean Albedo WAT: ',WAT_DF['albedo'].mean())\n",
    "    print('Std Albedo WAT: ',WAT_DF['albedo'].std())\n",
    "    print('Min Albedo WAT: ',WAT_DF['albedo'].min())\n",
    "    print('Max Albedo WAT: ',WAT_DF['albedo'].max())\n",
    "    print('WAT number of observations (n) = ', len(WAT_DF))\n",
    "    print()\n",
    "    print('Mean Albedo SN: ',SN_DF['albedo'].mean())\n",
    "    print('Std Albedo SN: ',SN_DF['albedo'].std())\n",
    "    print('Min Albedo SN: ',SN_DF['albedo'].min())\n",
    "    print('Max Albedo SN: ',SN_DF['albedo'].max())\n",
    "    print('SN number of observations (n) = ', len(SN_DF))    \n",
    "    \n",
    "    return HA_DF,LA_DF,CI_DF,CC_DF,WAT_DF,SN_DF\n",
    "\n",
    "    ################################################################################\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function\n",
    "HA_DF,LA_DF,CI_DF,CC_DF,WAT_DF,SN_DF = albedo_report_all_sites(albedoDFall,HA_DF1,LA_DF1,CI_DF1,CC_DF1,WAT_DF1,SN_DF1,HA_DF2,LA_DF2,CI_DF2,CC_DF2,WAT_DF2,SN_DF2,HA_DF3,LA_DF3,CI_DF3,CC_DF3,WAT_DF3,SN_DF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
